{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjUHAlB9Ykt9+Vay1cyqzz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVEZagBU1hFh",
        "outputId": "52dff245-6ce0-4392-ca99-c5af16eb1643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch safetensors pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "pdf_path = \"/content/LABOUR_ACT.pdf\"  # Update with your actual file path\n",
        "dataset_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "print(dataset_text[:1000])  # Print first 1000 characters to check output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV8FHaLI259T",
        "outputId": "67dca665-aa24-46ed-b1a2-579c32c1061b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Fix padding issue\n",
        "\n",
        "tokenized_data = tokenizer(\n",
        "    dataset_text,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(tokenized_data[\"input_ids\"].shape)  # Check if tokens are generated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "depobDu64UaK",
        "outputId": "a035ba1a-cfcd-4887-c0e7-352ac4aebafe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = tokenized_data[\"input_ids\"]\n",
        "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "        }\n",
        "\n",
        "tokenized_dataset = TextDataset(tokenized_data)\n",
        "print(len(tokenized_dataset))  # Should be > 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiqG2xk84sGZ",
        "outputId": "9b893dd1-b3c7-4d19-f9fa-0d00913d8df5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Resize the token embeddings because we added a PAD token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded and ready for training on\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uepm-i0y4y-b",
        "outputId": "69b1a75e-77a6-4d29-f1ea-28c7b01a8a79"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and ready for training on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned\",  # Directory to save the model\n",
        "    overwrite_output_dir=True,  # Overwrite previous training results\n",
        "    num_train_epochs=3,  # Number of training epochs (you can increase if needed)\n",
        "    per_device_train_batch_size=4,  # Batch size for training\n",
        "    save_steps=500,  # Save the model every 500 steps\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
        "    prediction_loss_only=True,\n",
        "    logging_dir=\"./logs\",  # Directory for logging training progress\n",
        ")\n",
        "\n",
        "print(\"Training arguments set!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ_ClOOI4_fC",
        "outputId": "76ff0702-88ad-4726-92ce-2d9069f98fe5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the dataset properly\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Define padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_list, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "\n",
        "        for text in text_list:\n",
        "            encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "            self.input_ids.append(encoding[\"input_ids\"].squeeze(0))  # Remove batch dimension\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.input_ids[idx]}\n",
        "\n",
        "# Ensure dataset_text is a list (if it's a single string, wrap it in a list)\n",
        "if isinstance(dataset_text, str):\n",
        "    dataset_text = [dataset_text]\n",
        "\n",
        "# Create tokenized dataset\n",
        "tokenized_dataset = TextDataset(dataset_text, tokenizer)\n",
        "\n",
        "print(\"Dataset tokenized and ready for training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM0OB77z5CYj",
        "outputId": "a9269061-662b-4b85-9f40-45e3fba502d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset tokenized and ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "M8KCxDIl61cj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,  # Reduce if memory issues\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\",  # Disable W&B if not needed\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewwindyi7L1o",
        "outputId": "0cd2d558-bedd-45dc-bf32-d2852dc75b95"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "RlaG4baR5VWe",
        "outputId": "7b493f0a-af5b-48df-a2ad-d1720c24d37d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=0.05162122845649719, metrics={'train_runtime': 7.519, 'train_samples_per_second': 0.399, 'train_steps_per_second': 0.399, 'total_flos': 783876096000.0, 'train_loss': 0.05162122845649719, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = tokenized_data[\"input_ids\"]\n",
        "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
        "        self.labels = tokenized_data[\"input_ids\"].clone()  # Labels must be identical to input_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],  # Add labels\n",
        "        }\n",
        "\n",
        "tokenized_dataset = TextDataset(tokenized_data)\n"
      ],
      "metadata": {
        "id": "tKyJiS5I8g73"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "-yaZkkRr8Tqg",
        "outputId": "e6531ad9-c396-47c0-eaa5-ea8e4f43567a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=4.354833602905273, metrics={'train_runtime': 10.4054, 'train_samples_per_second': 0.288, 'train_steps_per_second': 0.288, 'total_flos': 783876096000.0, 'train_loss': 4.354833602905273, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"Enter a sample prompt here\"\n",
        "output = generator(prompt, max_length=100)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myJDHJk69CC4",
        "outputId": "34ce11d1-287c-4a49-fb5a-19a485e04135"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Enter a sample prompt here\\n\\nAnAn\\n\\n1 hr. 1209\\n\\n11.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrtLdId-9KOR",
        "outputId": "058d1b74-8f14-4878-8295-2cb78e14dc64"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine_tuned_gpt2/tokenizer_config.json',\n",
              " 'fine_tuned_gpt2/special_tokens_map.json',\n",
              " 'fine_tuned_gpt2/vocab.json',\n",
              " 'fine_tuned_gpt2/merges.txt',\n",
              " 'fine_tuned_gpt2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJDRXHad98mv",
        "outputId": "ce30bd46-af7f-40f0-fe6d-94a7525d0733"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'fine_tuned_gpt2', 'LABOUR_ACT.pdf', 'results', 'gpt2_finetuned', 'logs', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_path = \"/content/fine_tuned_gpt2\"  # Update this if needed\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Move model to CUDA\n",
        "model.to(\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqBYGbM3-Ijr",
        "outputId": "b792afe5-73ac-47f4-988c-24629a11b2d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Set model path\n",
        "model_path = \"fine_tuned_gpt2\"\n",
        "\n",
        "# Load tokenizer\n",
        "try:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS token\n",
        "except Exception as e:\n",
        "    print(\"Error loading tokenizer:\", e)\n",
        "    exit()\n",
        "\n",
        "# Load model\n",
        "try:\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "except Exception as e:\n",
        "    print(\"Error loading model:\", e)\n",
        "    exit()\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_p=0.95,  # Balanced randomness\n",
        "        temperature=0.3,  # Reduces hallucination\n",
        "        repetition_penalty=2.0,  # Avoids repetition\n",
        "        do_sample=True,\n",
        "        num_beams=5  # Enhances factual correctness\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "prompt = \"According to the Nigerian Labour Act, employees are entitled to\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qm5wysJ9ntp",
        "outputId": "a3eb74f3-c4f7-4373-d146-1ec3a4485626"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the Nigerian Labour Act, employees are entitled to compensation of up to $1,000 for each day they work in Nigeria and up to $2,000 for each day they do not work in Nigeria.\n",
            "\n",
            "The law also states that if an employee does not work in Nigeria, he or she is entitled to compensation of up to $1,000 for each day he or she does not work in Nigeria.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"According to the Nigerian Labour Act, employees are entitled to\",\n",
        "    \"The rights of Nigerian workers include\",\n",
        "    \"Employers in Nigeria must provide\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Generated:\", generate_text(prompt))\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSDFcFzjB4In",
        "outputId": "71971c74-ec42-4a39-b8eb-2b977bb83981"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: According to the Nigerian Labour Act, employees are entitled to\n",
            "Generated: According to the Nigerian Labour Act, employees are entitled to compensation of up to $1,000 for each day they work in Nigeria and up to $2,000 for each day they do not work in Nigeria.\n",
            "\n",
            "The Nigerian Labour Act states that if a person is employed in Nigeria for more than one year, he or she may be entitled to compensation of up to $1,000 for each day they work in Nigeria and up to $2,000 for each day they do not work in Nigeria.\n",
            "--------------------------------------------------------------------------------\n",
            "Prompt: The rights of Nigerian workers include\n",
            "Generated: The rights of Nigerian workers include the right to self-determination and the right to a fair trial.\n",
            "\n",
            "\"We are very concerned about the situation in Nigeria,\" he said.\n",
            "--------------------------------------------------------------------------------\n",
            "Prompt: Employers in Nigeria must provide\n",
            "Generated: Employers in Nigeria must provide a minimum wage of at least $10,000 per month.\n",
            "\n",
            "The Nigerian government has said it will increase the minimum wage to $15 an hour by 2020.\n",
            "\n",
            "Nigerian President Muhammadu Buhari has vowed to raise the minimum wage to $15 an hour by 2022.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}